# Recurrent PPO Configuration for CybORG RL Training
# Proximal Policy Optimization with LSTM for partial observability

# Environment Configuration
fully_obs: false  # Partial observability (POMDP)
randomize_env: false

max_params:
  MAX_HOSTS: 5
  MAX_PROCESSES: 2
  MAX_CONNECTIONS: 2
  MAX_VULNERABILITIES: 1
  MAX_INTERFACES: 2
  MAX_SESSIONS: 3
  MAX_USERS: 5
  MAX_FILES: 0
  MAX_GROUPS: 0
  MAX_PATCHES: 0

# Default Hyperparameters
# These can be overridden by Terraform variables or command-line arguments
hyperparameters:
  # Core RL parameters
  gamma: 0.99
  learning_rate: 0.0001

  # PPO parameters
  n_steps: 1024           # Number of steps per rollout
  batch_size: 1024        # Minibatch size (must be multiple of n_steps * n_envs)
  n_epochs: 10            # Number of epochs per update
  clip_range: 0.1         # PPO clip range (0.2 is default, 0.1 more conservative)

  # Advantage estimation
  gae_lambda: 1.0         # GAE lambda (1.0 = no bias, 0.95 default)
  normalize_advantage: false  # Normalize advantages across batch

  # Loss coefficients
  ent_coef: 0.0           # Entropy coefficient for exploration
  vf_coef: 1.0            # Value function loss coefficient (0.5 default)

  # Early stopping
  target_kl: 0.01         # Target KL divergence for early stopping

# Network Architecture
network:
  type: lstm
  # Architecture is dynamically set to [input_size, input_size]
  # where input_size is calculated from observation space
  hidden_size: input  # Uses input size as hidden size
  lstm_num_layers: 2  # Number of LSTM layers

# Training Configuration
training:
  # These are typically set by Terraform or command line
  total_steps: 750000  # Total training timesteps (500K default for PPO)
  n_envs: 1            # Number of parallel environments
  checkpoint_freq: 10000  # Save checkpoint every N steps

  # Device configuration
  device: auto  # 'auto', 'cuda', or 'cpu'

# Scenario Configuration
# Default scenario file (can be overridden)
scenario: recurrent_ppo_scenario.yaml

# Notes:
# - This config uses partial observability (fully_obs: false) which makes the task harder
#   but is realistic for the Recurrent PPO architecture with LSTM
# - n_steps=1024 means the agent collects 1024 steps before each update
# - batch_size=1024 must be a multiple of n_steps * n_envs
# - clip_range=0.1 is more conservative than default 0.2
# - gae_lambda=1.0 uses full TD(lambda) with no bias (0.95 introduces bias but lower variance)
# - normalize_advantage=false matches the reference implementation
# - vf_coef=1.0 gives equal weight to value and policy loss (0.5 is default)
# - target_kl=0.01 enables early stopping if policy changes too much
